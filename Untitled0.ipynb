{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2ulZc39qkp1sclEr5rb6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjana-Savadatti/IBM_Project/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R26j8luvjGrV",
        "outputId": "41c17451-dfb1-4a1b-fe7c-770af6bd26c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8269943865ac>:77: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  predict_data['age'] = model.predict(X_predict)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score, mean_squared_error, confusion_matrix\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/My Drive/CRM_Data.xlsx'  # Update the path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Correct misspelled 'gmail' in email addresses\n",
        "def correct_gmail(email):\n",
        "    return re.sub(r'\\b(gmial|gmaill|gmai|gmal|gmaul|gmeil)\\.com\\b', 'gmail.com', str(email), flags=re.IGNORECASE)\n",
        "\n",
        "if 'email' in data.columns:\n",
        "    data['email'] = data['email'].str.lower().apply(correct_gmail)\n",
        "\n",
        "# Fill missing names using email prefix\n",
        "def extract_name_from_email(email):\n",
        "    if pd.isna(email):\n",
        "        return 'Unknown'\n",
        "    return email.split('@')[0] if '@' in email else 'Unknown'\n",
        "\n",
        "if 'name' in data.columns and 'email' in data.columns:\n",
        "    data['name'] = data['name'].fillna(data['email'].apply(extract_name_from_email))\n",
        "\n",
        "# Remove duplicates using TF-IDF & DBSCAN\n",
        "data['combined'] = data.get('name', '').fillna('') + ' ' + data.get('email', '').fillna('')\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(data['combined'])\n",
        "\n",
        "eps_values = np.linspace(0.3, 1.0, 5)\n",
        "best_eps = None\n",
        "best_score = -1\n",
        "for eps in eps_values:\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=2, metric='cosine')\n",
        "    clusters = dbscan.fit_predict(X)\n",
        "    if len(set(clusters)) > 1:\n",
        "        score = silhouette_score(X, clusters)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_eps = eps\n",
        "\n",
        "data['cluster'] = DBSCAN(eps=best_eps, min_samples=2, metric='cosine').fit_predict(X)\n",
        "\n",
        "# Missing Value Imputation with Linear Regression & KNN\n",
        "if 'age' in data.columns:\n",
        "    train_data = data[data['age'].notnull()]\n",
        "    predict_data = data[data['age'].isnull()]\n",
        "\n",
        "    X_train = pd.get_dummies(train_data.drop(columns=['age', 'combined', 'cluster'], errors='ignore'), drop_first=True)\n",
        "    y_train = train_data['age']\n",
        "\n",
        "    # Handle missing values in features before training\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    X_predict = pd.get_dummies(predict_data.drop(columns=['age', 'combined', 'cluster'], errors='ignore'), drop_first=True)\n",
        "    X_predict = X_predict.reindex(columns=X_train.columns, fill_value=0)\n",
        "    X_predict = pd.DataFrame(imputer.transform(X_predict), columns=X_predict.columns)\n",
        "\n",
        "    predict_data['age'] = model.predict(X_predict)\n",
        "\n",
        "    data = pd.concat([train_data, predict_data])\n",
        "\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "data[data.select_dtypes(include=['float64', 'int64']).columns] = knn_imputer.fit_transform(\n",
        "    data.select_dtypes(include=['float64', 'int64'])\n",
        ")\n",
        "\n",
        "# Anomaly Detection using Decision Tree & Logistic Regression\n",
        "if 'valid' in data.columns:\n",
        "    X = pd.get_dummies(data.drop(columns=['valid', 'combined', 'cluster'], errors='ignore'), drop_first=True)\n",
        "    y = data['valid']\n",
        "\n",
        "    # Handle missing values in features before training\n",
        "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "    dt_clf.fit(X_train, y_train)\n",
        "    dt_predictions = dt_clf.predict(X_test)\n",
        "\n",
        "    log_reg = LogisticRegression()\n",
        "    log_reg.fit(X_train, y_train)\n",
        "    log_predictions = log_reg.predict(X_test)\n",
        "\n",
        "    cm = confusion_matrix(y_test, dt_predictions)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Save the cleaned and enriched data\n",
        "cleaned_file_path = '/content/drive/My Drive/cleaned_data.xlsx'\n",
        "data.to_excel(cleaned_file_path, index=False)"
      ]
    }
  ]
}